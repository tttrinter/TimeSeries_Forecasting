---
title: "ARIMA Revenue Forecasting"
author: "Tom Trinter"
date: "December 9, 2019"
# output:
  # word_document: default
  # html_document: default
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(ggplot2)
library(gridExtra)
library(scales)
library(xlsx)
library(data.table)
library(fpp2)
library(fUnitRoots)
library(dplyr)

```

# Revenue Forecasting - ARIMA Modeling

## Objective
The objective of this is to model product revenues to better inform their forecasting process. This is derived from a Fortune 500 client project where we had three years of monthly actual revenue data across 45 different products. These produts are grouped into 6 product groups. The data has been sanitized and product names genericized to protect the proprietary nature of the data and client privacy.

We'll begin by modeling the revenues at the product-group level using simple ARIMA models. We'll then look at modeling at the individual product level. 

## Data

For this analysis, we're starting with a spreadsheet where the data was maintained manually for years in the treasury department. I'll convert the data from its 'wide' form into 'long' for easier use going forward.

#### Documentation

This approach follows the example found [here](https://datascienceplus.com/time-series-analysis-using-arima-model-in-r/).

```{r data}
working.dir <- '/Users/tttri/git_repo/RevenueForecasting/'
setwd(working.dir)

long.filename <- 'Revenue Data - Long.xlsx'
pd <- read.xlsx(long.filename, sheetName = 'forecast_data')

# Reduce to actuals only (forecast_id=0)
pd <- pd %>% 
  subset(forecast_id==0 & month_start>='2017-04-01') %>% 
  mutate(value_m = value/1000000)

```

## Plot Product Groups

```{r Product_Groups}

# Let's plot each product_group vs. time
plot.data <- pd %>%
  select(summary_display, month_start, value_m) %>% 
  group_by(summary_display, month_start) %>%
  summarize(group_value = sum(value_m,na.rm=T)) %>% 
  filter(!is.na(summary_display) & month_start < '2019-07-01')

p <- 
  ggplot(plot.data,
         aes(x=month_start, y=group_value)) + 
  geom_line() + 
  geom_point() +
  geom_smooth(se=F)+
  labs(x="Month",
       y="Value ($MM)",
       title="Monthly Revenue")+
  theme_bw() + 
  facet_wrap(~summary_display, scales="free") 
p
```

Reviewing these plots to ensure that the transformations of the data to remove any client reference resulted in usable data for the remainder of the analysis. I'm looking for reasonably smooth values and generally consistent order of magnitude.

## ARIMA Modeling

Looking at the time-series plots of grouped product revenues, there seem to be some potential trends to extract before we can use an ARIMA model. Let's take a closer look at the Annuity data and add in a 1st difference to see if that will get closer to stationary.

```{r Brass_Data}
brass.data <- pd %>%
  filter(summary_display=='Brass' & month_start < '2022-06-01' ) %>% 
  group_by(month_start) %>%
  summarize(group_value = sum(value_m,na.rm=T)) %>% 
  select(month_start, group_value)

ts.data <- ts(brass.data$group_value, frequency=12)

ggtsdisplay(ts.data, main="Brass Revenue")

```

Here I'm looking to see if the data is stationary. Since the bars in ACF and PACF don't cross the blue threshhold lines, I think we're OK. Also, using the auto.arima method, the models are auto-fit to different values of autocorrelation, so it should be handled there as well.

## Components

```{r Components}
# components.ts <- decompose(ts.data)
components.ts <- stl(ts.data, s.window=12)
plot(components.ts)

```

## Adjust for seasonality

From the decomposition plot above, it looks like there is some quarterly seasonality as well as an overall decreasing trend. 

So - we can remove both of those and fit a regression to them separately? For now, just removing the trend...
```{r}
ts.data.adj <- ts.data - components.ts$time.series[,2]
```

## Unit Root Test:
After removing the trend component of the time series, let's check if the time series is stationary again:
```{r unitRoots}
urkpssTest(ts.data.adj, type=c("tau"), lags = c("short"), use.lag=NULL, doplot=TRUE)

tsstationary <- diff(ts.data.adj, differences=1)
plot(tsstationary)

```

Since it is centered around zero, we can move on to modeling the remaining timeseries. I'll use `auto.arima` to fit a range of models to the data, exploring different values for auto-regression and differencing.

```{r Arima}
fit <- auto.arima(ts.data.adj, trace = TRUE)

```

Checking the residuals from the final ARIMA model - we're looking for residuals centered around zero and a normally shaped distribution.
```{r}
checkresiduals(fit)
```
## Create a Product Group Time-series Foreacast
Now that we have the time-series fit to an ARIMA model, we can forecast forward for this product group.
```{r}
brass.forecast <- forecast(fit)
autoplot(brass.forecast)
```

## Model the trend component
[Example LM](https://datascienceplus.com/fitting-polynomial-regression-r/)
```{r}
 trend.data <- data.frame('days' = brass.data$month_start-brass.data$month_start[1],
                         trend_val = components.ts$time.series[,2] ) %>% 
  mutate(days = as.numeric(days))

simple.fit <- lm(trend_val ~ poly(days, 4), data= trend.data)

summary(simple.fit)

trend.data$predicted <- simple.fit$fitted.values

ggplot(trend.data, aes(days, y = trend_val, color = variable)) + 
    geom_line(aes(y = trend_val, col = "Actual"), size=1) + 
    geom_line(aes(y = predicted, col = "Predicted"), size=1)

```

## Final Forecast
The final forecast is the combination of the forecasted time-series and the forecasted trend. I'll plot the actuals vs. forecasted to get an idea of overall model fit.
```{r}
brass.data$predicted <-simple.fit$fitted.values + brass.forecast$model$fitted

ggplot(brass.data, aes(month_start, y = group_value, color = variable)) + 
    geom_line(aes(y = group_value, col = "Actual"), size=1) + 
    geom_line(aes(y = predicted, col = "Predicted"), size=1)

```

## Next Steps
Now that I've worked through one product group, there is more to be done. Specifically, looking at individual product level forecasts, and evaluating the model on test data that is outside of the data range used to fit the models.
